"""
End-to-End test for complete RAG workflow with semantic search and metadata filtering

Tests the full lifecycle with thematic documents:
1. Health check (API + current documents)
2. Upload 9 documents with distinct topics AND metadata (multi-user simulation)
   - TXT: RAG technology (alice@company.com, engineering, python/api)
   - PDF: AI agents (alice@company.com, engineering, agents/quality)
   - MD: Databases (bob@company.com, engineering, database/postgresql)
   - JSON: Products (bob@company.com, sales, catalog/pricing)
   - HTML: Art (charlie@company.com, marketing, exhibitions/events)
   - YAML: Business (charlie@company.com, finance, metrics/kpis)
   - XML: GDPR (alice@company.com, legal, compliance/gdpr)
   - CSV: Financials (bob@company.com, finance, quarterly/reports)
   - LOG: Operations (charlie@company.com, operations, logs/monitoring)
3. List documents
4. Metadata filtering tests - validate MongoDB-style filters work correctly
   - User isolation (uploaded_by filtering)
   - Department filtering (engineering, sales, legal, finance, marketing, operations)
   - Tag matching ($in operator)
   - Priority filtering ($ne operator)
   - Complex AND/OR/NOT combinations
   - $exists operator
5. Semantic search tests - validate RAG returns topically correct documents
   - Product queries â†’ electronics catalog
   - Art queries â†’ exhibition info
   - Business queries â†’ metrics
   - Compliance queries â†’ GDPR report
   - Financial queries â†’ quarterly report
   - Operations queries â†’ system logs
   - Topic isolation (negative test)
6. Download original files
7. Delete documents
8. Verify cleanup (DB + GCS)

Philosophy: Non-trivial queries test semantic understanding vs keyword matching.
Each document has distinct topic AND metadata, enabling validation that both
semantic search and metadata filtering work correctly in production scenarios.
"""

import json
import os
import tempfile
from pathlib import Path

import pytest
import requests
from google.cloud import storage

from src.utils import calculate_file_hash


# Test configuration
API_BASE = "http://localhost:8080"
GCS_BUCKET = "myai-475419-rag-documents"


@pytest.fixture(scope="module")
def gcs_client():
    """GCS client for storage verification"""
    return storage.Client()


@pytest.fixture(scope="module")
def test_documents():
    """Test document paths with distinct topics for semantic search validation"""
    fixtures_dir = Path(__file__).parent.parent / "fixtures" / "documents"
    docs = {
        # Each document has a distinct topic for semantic testing
        "txt": fixtures_dir / "rag_architecture_guide.txt",  # Topic: RAG technology
        "pdf": fixtures_dir / "google_agent_quality.pdf",  # Topic: AI agents, software engineering
        "md": fixtures_dir / "vector_databases.md",  # Topic: Databases, vector search
        "json": fixtures_dir / "electronics_catalog.json",  # Topic: Products, prices, specifications
        "html": fixtures_dir / "art_exhibition.html",  # Topic: Art, exhibitions, tickets
        "yaml": fixtures_dir / "business_metrics.yaml",  # Topic: Business KPIs, metrics
        "xml": fixtures_dir / "gdpr_compliance.xml",  # Topic: Legal, GDPR, compliance
        "csv": fixtures_dir / "financial_quarterly_report.csv",  # Topic: Financial reports
        "log": fixtures_dir / "rag_system_operations.log",  # Topic: System operations, logs
        "security": fixtures_dir / "security_test.txt",  # Topic: Security test (protected fields)
        "technical": fixtures_dir / "hybrid_search_technical.txt",  # Topic: Hybrid search algorithms (technical)
        "story": fixtures_dir / "red_riding_hood_story.txt",  # Topic: Fairy tale (keyword trap for reranking tests)
    }
    
    # Calculate hashes for selective cleanup (preserves user documents)
    docs["txt_hash"] = calculate_file_hash(docs["txt"])
    docs["pdf_hash"] = calculate_file_hash(docs["pdf"])
    docs["md_hash"] = calculate_file_hash(docs["md"])
    docs["json_hash"] = calculate_file_hash(docs["json"])
    docs["html_hash"] = calculate_file_hash(docs["html"])
    docs["yaml_hash"] = calculate_file_hash(docs["yaml"])
    docs["xml_hash"] = calculate_file_hash(docs["xml"])
    docs["csv_hash"] = calculate_file_hash(docs["csv"])
    docs["log_hash"] = calculate_file_hash(docs["log"])
    docs["security_hash"] = calculate_file_hash(docs["security"])
    docs["technical_hash"] = calculate_file_hash(docs["technical"])
    docs["story_hash"] = calculate_file_hash(docs["story"])
    
    return docs


@pytest.mark.cleanup
@pytest.mark.e2e
def test_00_cleanup_before(test_documents, auth_headers):
    """Step 0: Pre-cleanup - Remove test documents from previous runs"""
    print("\n=== Step 0: Pre-cleanup (removes leftover test documents) ===")
    print("ðŸ§¹ Cleaning up test documents from previous runs...")
    print("   (user documents will be preserved)")
    
    # Delete test documents by hash (if they exist from previous run)
    deleted = 0
    for hash_key in ["txt_hash", "pdf_hash", "md_hash", "json_hash", "html_hash", "yaml_hash", "xml_hash", "csv_hash", "log_hash", "security_hash", "technical_hash", "story_hash"]:
        file_hash = test_documents[hash_key]
        response = requests.delete(f"{API_BASE}/v1/documents/by-hash/{file_hash}", headers=auth_headers, timeout=30)
        if response.status_code == 200:
            result = response.json()
            print(f"   âœ“ Deleted leftover: {result['filename']}")
            deleted += 1
    
    if deleted == 0:
        print("   âœ“ No leftover test documents found")
    else:
        print(f"   âœ“ Deleted {deleted} leftover test document(s)")
    
    print("âœ“ Ready to run tests (user documents preserved)\n")


@pytest.mark.e2e
def test_01_verify_api_health():
    """Step 1: Verify API is running and healthy"""
    print("\n=== Step 1: Verify API health ===")
    
    # Check API health
    response = requests.get(f"{API_BASE}/health", timeout=10)
    assert response.status_code == 200
    health = response.json()
    print(f"âœ“ API version: {health['version']}, uptime: {health['uptime_seconds']}s")
    
    # Check API can list documents (may have user documents - that's OK!)
    response = requests.get(f"{API_BASE}/v1/documents", timeout=10)
    assert response.status_code == 200
    docs = response.json()
    print(f"âœ“ API responsive, current documents: {docs['total']}")


@pytest.mark.upload
@pytest.mark.e2e
def test_02_upload_txt_document(test_documents, auth_headers):
    """Step 2: Upload RAG architecture guide (Alice, engineering, Python/API)"""
    print("\n=== Step 2: Upload RAG architecture guide (TXT) ===")
    
    txt_path = test_documents["txt"]
    assert txt_path.exists(), f"Test file not found: {txt_path}"
    
    # Alice (engineering, Python developer)
    headers = {**auth_headers, "X-End-User-ID": "alice@company.com"}
    metadata = json.dumps({
        "category": "e2e-fixture",
        "department": "engineering",
        "tags": ["python", "api", "architecture", "rag"],
        "priority": "high"
    })
    
    with open(txt_path, "rb") as f:
        files = {"file": (txt_path.name, f, "text/plain")}
        data = {"metadata": metadata}
        response = requests.post(f"{API_BASE}/v1/documents/upload", files=files, data=data, headers=headers, timeout=60)
    
    assert response.status_code == 200, f"Upload failed: {response.text}"
    result = response.json()
    
    assert result["chunks_created"] > 0, "No chunks created"
    print(f"âœ“ Uploaded: {result['filename']} (alice@company.com)")
    print(f"  - ID: {result['doc_id']}, UUID: {result['doc_uuid']}")
    print(f"  - Chunks: {result['chunks_created']}")
    print(f"  - Metadata: engineering, tags=[python, api, architecture, rag], priority=high")


@pytest.mark.upload
@pytest.mark.e2e
def test_02a_upload_technical_doc(test_documents, auth_headers):
    """Step 2a: Upload hybrid search technical doc (Alice, engineering)"""
    print("\n=== Step 2a: Upload hybrid search technical documentation (TXT) ===")
    
    technical_path = test_documents["technical"]
    assert technical_path.exists(), f"Test file not found: {technical_path}"
    
    headers = {**auth_headers, "X-End-User-ID": "alice@company.com"}
    metadata = json.dumps({
        "category": "e2e-fixture",
        "department": "engineering",
        "tags": ["hybrid-search", "algorithms", "rag", "technical"],
        "priority": "high"
    })
    
    with open(technical_path, "rb") as f:
        files = {"file": (technical_path.name, f, "text/plain")}
        data = {"metadata": metadata}
        response = requests.post(f"{API_BASE}/v1/documents/upload", files=files, data=data, headers=headers, timeout=60)
    
    assert response.status_code == 200, f"Upload failed: {response.text}"
    result = response.json()
    
    assert result["chunks_created"] > 0, "No chunks created"
    print(f"âœ“ Uploaded: {result['filename']} (alice@company.com)")
    print(f"  - ID: {result['doc_id']}, UUID: {result['doc_uuid']}")
    print(f"  - Chunks: {result['chunks_created']}")
    print(f"  - Metadata: engineering, tags=[hybrid-search, algorithms, rag, technical]")


@pytest.mark.upload
@pytest.mark.e2e
def test_02b_upload_story_doc(test_documents, auth_headers):
    """Step 2b: Upload Red Riding Hood story (keyword trap for reranking tests)"""
    print("\n=== Step 2b: Upload Red Riding Hood story (TXT) - keyword trap ===")
    
    story_path = test_documents["story"]
    assert story_path.exists(), f"Test file not found: {story_path}"
    
    headers = {**auth_headers, "X-End-User-ID": "alice@company.com"}
    metadata = json.dumps({
        "category": "e2e-fixture",
        "department": "entertainment",
        "tags": ["story", "fiction", "fairy-tale"],
        "priority": "low"
    })
    
    with open(story_path, "rb") as f:
        files = {"file": (story_path.name, f, "text/plain")}
        data = {"metadata": metadata}
        response = requests.post(f"{API_BASE}/v1/documents/upload", files=files, data=data, headers=headers, timeout=60)
    
    assert response.status_code == 200, f"Upload failed: {response.text}"
    result = response.json()
    
    assert result["chunks_created"] > 0, "No chunks created"
    print(f"âœ“ Uploaded: {result['filename']} (alice@company.com)")
    print(f"  - ID: {result['doc_id']}, UUID: {result['doc_uuid']}")
    print(f"  - Chunks: {result['chunks_created']}")
    print(f"  - Metadata: entertainment, tags=[story, fiction, fairy-tale]")


@pytest.mark.upload
@pytest.mark.e2e
def test_03_upload_pdf_document(test_documents, auth_headers):
    """Step 3: Upload AI agent quality guide (Alice, engineering, agents/quality)"""
    print("\n=== Step 3: Upload AI agent quality guide (PDF) ===")
    
    pdf_path = test_documents["pdf"]
    assert pdf_path.exists(), f"Test file not found: {pdf_path}"
    
    # Alice (engineering, AI/agents focus)
    headers = {**auth_headers, "X-End-User-ID": "alice@company.com"}
    metadata = json.dumps({
        "category": "e2e-fixture",
        "department": "engineering",
        "tags": ["agents", "quality", "ai", "software-engineering"],
        "priority": "high"
    })
    
    with open(pdf_path, "rb") as f:
        files = {"file": (pdf_path.name, f, "application/pdf")}
        data = {"metadata": metadata}
        response = requests.post(f"{API_BASE}/v1/documents/upload", files=files, data=data, headers=headers, timeout=60)
    
    assert response.status_code == 200, f"Upload failed: {response.text}"
    result = response.json()
    
    assert result["chunks_created"] > 0, "No chunks created"
    print(f"âœ“ Uploaded: {result['filename']} (alice@company.com)")
    print(f"  - ID: {result['doc_id']}, UUID: {result['doc_uuid']}")
    print(f"  - Chunks: {result['chunks_created']}")
    print(f"  - Metadata: engineering, tags=[agents, quality, ai, software-engineering], priority=high")


@pytest.mark.upload
@pytest.mark.e2e
def test_03b_upload_markdown_document(test_documents, auth_headers):
    """Step 3b: Upload vector databases guide (Bob, engineering, database/postgresql)"""
    print("\n=== Step 3b: Upload vector databases guide (Markdown) ===")
    
    md_path = test_documents["md"]
    assert md_path.exists(), f"Test file not found: {md_path}"
    
    # Bob (engineering, database specialist)
    headers = {**auth_headers, "X-End-User-ID": "bob@company.com"}
    metadata = json.dumps({
        "category": "e2e-fixture",
        "department": "engineering",
        "tags": ["database", "postgresql", "vector-search", "pgvector"],
        "priority": "high"
    })
    
    with open(md_path, "rb") as f:
        files = {"file": (md_path.name, f, "text/markdown")}
        data = {"metadata": metadata}
        response = requests.post(f"{API_BASE}/v1/documents/upload", files=files, data=data, headers=headers, timeout=60)
    
    assert response.status_code == 200, f"Upload failed: {response.text}"
    result = response.json()
    
    assert result["chunks_created"] > 0, "No chunks created"
    print(f"âœ“ Uploaded: {result['filename']} (bob@company.com)")
    print(f"  - ID: {result['doc_id']}, UUID: {result['doc_uuid']}")
    print(f"  - Chunks: {result['chunks_created']}")
    print(f"  - Metadata: engineering, tags=[database, postgresql, vector-search, pgvector], priority=high")


@pytest.mark.upload
@pytest.mark.e2e
def test_03c_upload_json_document(test_documents, auth_headers):
    """Step 3c: Upload electronics catalog (Bob, sales, catalog/pricing)"""
    print("\n=== Step 3c: Upload electronics catalog (JSON â†’ YAML) ===")
    
    json_path = test_documents["json"]
    assert json_path.exists(), f"Test file not found: {json_path}"
    
    # Bob (sales, product catalog)
    headers = {**auth_headers, "X-End-User-ID": "bob@company.com"}
    metadata = json.dumps({
        "category": "e2e-fixture",
        "department": "sales",
        "tags": ["catalog", "products", "pricing", "electronics"],
        "priority": "medium"
    })
    
    with open(json_path, "rb") as f:
        files = {"file": (json_path.name, f, "application/json")}
        data = {"metadata": metadata}
        response = requests.post(f"{API_BASE}/v1/documents/upload", files=files, data=data, headers=headers, timeout=60)
    
    assert response.status_code == 200, f"Upload failed: {response.text}"
    result = response.json()
    
    assert result["chunks_created"] > 0, "No chunks created from JSON (YAML conversion)"
    print(f"âœ“ Uploaded: {result['filename']} (bob@company.com)")
    print(f"  - ID: {result['doc_id']}, UUID: {result['doc_uuid']}")
    print(f"  - Chunks: {result['chunks_created']}")
    print(f"  - Metadata: sales, tags=[catalog, products, pricing, electronics], priority=medium")
    print("  - Note: JSON converted to YAML for semantic preservation")


@pytest.mark.security
@pytest.mark.e2e
def test_03c2_security_protected_metadata_fields(test_documents, auth_headers):
    """Step 3c2: SECURITY - User cannot override protected system metadata fields"""
    print("\n=== Step 3c2: SECURITY TEST - Protected metadata fields ===")
    
    security_path = test_documents["security"]
    assert security_path.exists(), f"Test file not found: {security_path}"
    
    # Attacker tries to impersonate another user and manipulate timestamps
    # These fields should be REJECTED (400 error) not silently filtered
    malicious_metadata = json.dumps({
        "uploaded_by": "admin@company.com",  # ATTACK: impersonation
        "uploaded_at": "2020-01-01T00:00:00",  # ATTACK: fake timestamp
        "uploaded_via": "trusted_service",  # ATTACK: fake source
        "original_filename": "secret.pdf",  # ATTACK: fake filename
        "department": "security",  # LEGITIMATE: user field
    })
    
    with open(security_path, "rb") as f:
        files = {"file": (security_path.name, f, "text/plain")}
        data = {"metadata": malicious_metadata}
        response = requests.post(f"{API_BASE}/v1/documents/upload", files=files, data=data, headers=auth_headers, timeout=60)
    
    # SECURITY VERIFICATION: Upload MUST be REJECTED with 400 error
    assert response.status_code == 400, f"Expected 400 error, got {response.status_code}: {response.text}"
    
    error_detail = response.json()["detail"]
    assert "protected field names" in error_detail.lower(), f"Error should mention protected fields: {error_detail}"
    assert "uploaded_by" in error_detail, "Error should list 'uploaded_by' as protected"
    assert "uploaded_at" in error_detail, "Error should list 'uploaded_at' as protected"
    assert "uploaded_via" in error_detail, "Error should list 'uploaded_via' as protected"
    
    print(f"âœ“ SECURITY VERIFIED: Upload rejected with 400 error")
    print(f"  - Error: {error_detail[:100]}...")
    
    # Now test that upload WITHOUT protected fields succeeds
    print(f"\n  Testing legitimate upload (no protected fields)...")
    legitimate_metadata = json.dumps({
        "category": "e2e-fixture",
        "department": "security",  # LEGITIMATE: user field
        "tags": ["test", "security"],
    })
    
    with open(security_path, "rb") as f:
        files = {"file": (security_path.name, f, "text/plain")}
        data = {"metadata": legitimate_metadata}
        response = requests.post(f"{API_BASE}/v1/documents/upload", files=files, data=data, headers=auth_headers, timeout=60)
    
    assert response.status_code == 200, f"Legitimate upload failed: {response.text}"
    result = response.json()
    doc_id = result["doc_id"]
    
    # Verify system fields are auto-populated correctly
    response = requests.get(f"{API_BASE}/v1/documents", headers=auth_headers, timeout=30)
    assert response.status_code == 200
    
    documents = response.json()["documents"]
    doc = next((d for d in documents if d["doc_id"] == doc_id), None)
    assert doc is not None, f"Document {doc_id} not found"
    
    # Check system fields at top level
    assert doc["uploaded_by"] == "javaisforever@gmail.com", "uploaded_by should be from JWT"
    assert doc["uploaded_at"].startswith("2025"), "uploaded_at should be current timestamp"
    assert doc["uploaded_via"] == "api", "uploaded_via should be 'api'"
    
    # Check metadata does NOT contain protected fields
    metadata = doc["metadata"]
    assert "uploaded_by" not in metadata, "metadata should not contain uploaded_by"
    assert "uploaded_at" not in metadata, "metadata should not contain uploaded_at"
    assert "uploaded_via" not in metadata, "metadata should not contain uploaded_via"
    
    # Check legitimate user fields are preserved
    assert metadata.get("department") == "security", "User field should be preserved"
    assert "test" in metadata.get("tags", []), "User field should be preserved"
    
    print(f"âœ“ Legitimate upload succeeded")
    print(f"  - uploaded_by: {doc['uploaded_by']} (from JWT)")
    print(f"  - uploaded_at: {doc['uploaded_at'][:19]}")
    print(f"  - uploaded_via: {doc['uploaded_via']}")
    print(f"  - metadata: {metadata} (no protected fields)")
    print(f"  âœ“ Protected fields CANNOT be overridden by user input")


@pytest.mark.upload
@pytest.mark.e2e
def test_03d_upload_html_document(test_documents, auth_headers):
    """Step 3d: Upload art exhibition info (Charlie, marketing, exhibitions/events)"""
    print("\n=== Step 3d: Upload art exhibition info (HTML â†’ Markdown) ===")
    
    html_path = test_documents["html"]
    assert html_path.exists(), f"Test file not found: {html_path}"
    
    # Charlie (marketing, events)
    headers = {**auth_headers, "X-End-User-ID": "charlie@company.com"}
    metadata = json.dumps({
        "category": "e2e-fixture",
        "department": "marketing",
        "tags": ["exhibitions", "events", "art", "tickets"],
        "priority": "low"
    })
    
    with open(html_path, "rb") as f:
        files = {"file": (html_path.name, f, "text/html")}
        data = {"metadata": metadata}
        response = requests.post(f"{API_BASE}/v1/documents/upload", files=files, data=data, headers=headers, timeout=60)
    
    assert response.status_code == 200, f"Upload failed: {response.text}"
    result = response.json()
    
    assert result["chunks_created"] > 0, "No chunks created from HTML"
    print(f"âœ“ Uploaded: {result['filename']} (charlie@company.com)")
    print(f"  - ID: {result['doc_id']}, UUID: {result['doc_uuid']}")
    print(f"  - Chunks: {result['chunks_created']}")
    print(f"  - Metadata: marketing, tags=[exhibitions, events, art, tickets], priority=low")
    print("  - Note: HTML converted to Markdown (preserves structure)")


@pytest.mark.upload
@pytest.mark.e2e
def test_03e_upload_yaml_document(test_documents, auth_headers):
    """Step 3e: Upload business metrics (Charlie, finance, metrics/kpis)"""
    print("\n=== Step 3e: Upload business metrics (YAML) ===")
    
    yaml_path = test_documents["yaml"]
    assert yaml_path.exists(), f"Test file not found: {yaml_path}"
    
    # Charlie (finance, business metrics)
    headers = {**auth_headers, "X-End-User-ID": "charlie@company.com"}
    metadata = json.dumps({
        "category": "e2e-fixture",
        "department": "finance",
        "tags": ["metrics", "kpis", "business", "performance"],
        "priority": "high"
    })
    
    with open(yaml_path, "rb") as f:
        files = {"file": (yaml_path.name, f, "application/x-yaml")}
        data = {"metadata": metadata}
        response = requests.post(f"{API_BASE}/v1/documents/upload", files=files, data=data, headers=headers, timeout=60)
    
    assert response.status_code == 200, f"Upload failed: {response.text}"
    result = response.json()
    
    assert result["chunks_created"] > 0, "No chunks created from YAML"
    print(f"âœ“ Uploaded: {result['filename']} (charlie@company.com)")
    print(f"  - ID: {result['doc_id']}, UUID: {result['doc_uuid']}")
    print(f"  - Chunks: {result['chunks_created']}")
    print(f"  - Metadata: finance, tags=[metrics, kpis, business, performance], priority=high")
    print("  - Note: YAML kept as-is (already optimal for LLM)")


@pytest.mark.upload
@pytest.mark.e2e
def test_03f_upload_xml_document(test_documents, auth_headers):
    """Step 3f: Upload GDPR compliance report (Alice, legal, compliance/gdpr)"""
    print("\n=== Step 3f: Upload GDPR compliance report (XML â†’ YAML) ===")
    
    xml_path = test_documents["xml"]
    assert xml_path.exists(), f"Test file not found: {xml_path}"
    
    # Alice (legal, compliance)
    headers = {**auth_headers, "X-End-User-ID": "alice@company.com"}
    metadata = json.dumps({
        "category": "e2e-fixture",
        "department": "legal",
        "tags": ["compliance", "gdpr", "legal", "privacy"],
        "priority": "high"
    })
    
    with open(xml_path, "rb") as f:
        files = {"file": (xml_path.name, f, "application/xml")}
        data = {"metadata": metadata}
        response = requests.post(f"{API_BASE}/v1/documents/upload", files=files, data=data, headers=headers, timeout=60)
    
    assert response.status_code == 200, f"Upload failed: {response.text}"
    result = response.json()
    
    assert result["chunks_created"] > 0, "No chunks created from XML (YAML conversion)"
    print(f"âœ“ Uploaded: {result['filename']} (alice@company.com)")
    print(f"  - ID: {result['doc_id']}, UUID: {result['doc_uuid']}")
    print(f"  - Chunks: {result['chunks_created']}")
    print(f"  - Metadata: legal, tags=[compliance, gdpr, legal, privacy], priority=high")
    print("  - Note: XML converted to YAML for semantic preservation")


@pytest.mark.upload
@pytest.mark.e2e
def test_03g_upload_csv_document(test_documents, auth_headers):
    """Step 3g: Upload financial quarterly report (Bob, finance, quarterly/reports)"""
    print("\n=== Step 3g: Upload financial quarterly report (CSV) ===")
    
    csv_path = test_documents["csv"]
    assert csv_path.exists(), f"Test file not found: {csv_path}"
    
    # Bob (finance, reports)
    headers = {**auth_headers, "X-End-User-ID": "bob@company.com"}
    metadata = json.dumps({
        "category": "e2e-fixture",
        "department": "finance",
        "tags": ["quarterly", "reports", "financial", "revenue"],
        "priority": "high"
    })
    
    with open(csv_path, "rb") as f:
        files = {"file": (csv_path.name, f, "text/csv")}
        data = {"metadata": metadata}
        response = requests.post(f"{API_BASE}/v1/documents/upload", files=files, data=data, headers=headers, timeout=60)
    
    assert response.status_code == 200, f"Upload failed: {response.text}"
    result = response.json()
    
    assert result["chunks_created"] > 0, "No chunks created from CSV"
    print(f"âœ“ Uploaded: {result['filename']} (bob@company.com)")
    print(f"  - ID: {result['doc_id']}, UUID: {result['doc_uuid']}")
    print(f"  - Chunks: {result['chunks_created']}")
    print(f"  - Metadata: finance, tags=[quarterly, reports, financial, revenue], priority=high")
    print("  - Note: CSV kept as plain text (tabular structure preserved)")


@pytest.mark.upload
@pytest.mark.e2e
def test_03h_upload_log_document(test_documents, auth_headers):
    """Step 3h: Upload system operations log (Charlie, operations, logs/monitoring)"""
    print("\n=== Step 3h: Upload system operations log (LOG) ===")
    
    log_path = test_documents["log"]
    assert log_path.exists(), f"Test file not found: {log_path}"
    
    # Charlie (operations, monitoring)
    headers = {**auth_headers, "X-End-User-ID": "charlie@company.com"}
    metadata = json.dumps({
        "category": "e2e-fixture",
        "department": "operations",
        "tags": ["logs", "monitoring", "system", "operations"],
        "priority": "medium"
    })
    
    with open(log_path, "rb") as f:
        files = {"file": (log_path.name, f, "text/plain")}
        data = {"metadata": metadata}
        response = requests.post(f"{API_BASE}/v1/documents/upload", files=files, data=data, headers=headers, timeout=60)
    
    assert response.status_code == 200, f"Upload failed: {response.text}"
    result = response.json()
    
    assert result["chunks_created"] > 0, "No chunks created from LOG"
    print(f"âœ“ Uploaded: {result['filename']} (charlie@company.com)")
    print(f"  - ID: {result['doc_id']}, UUID: {result['doc_uuid']}")
    print(f"  - Chunks: {result['chunks_created']}")
    print(f"  - Metadata: operations, tags=[logs, monitoring, system, operations], priority=medium")
    print("  - Note: Log file kept as plain text (timestamps + messages)")


@pytest.mark.list
@pytest.mark.e2e
def test_04_list_documents():
    """Step 4: Verify documents are listed"""
    print("\n=== Step 4: List all documents ===")
    
    response = requests.get(f"{API_BASE}/v1/documents", timeout=10)
    assert response.status_code == 200
    
    docs = response.json()
    assert docs["total"] >= 1, "Expected at least 1 document"
    
    print(f"âœ“ Found {docs['total']} document(s):")
    for doc in docs["documents"]:
        print(f"  - [{doc['doc_id']}] {doc['filename']} ({doc['chunk_count']} chunks)")


@pytest.mark.metadata_filter
@pytest.mark.e2e
def test_04a_metadata_filter_by_user(auth_headers):
    """Step 4a: Metadata filtering - user isolation (multi-tenant)"""
    print("\n=== Step 4a: Metadata filtering - User isolation ===")
    
    # Alice's documents only
    payload = {
        "query": "technical documentation",
        "filters": {"uploaded_by": "alice@company.com"},
        "top_k": 10
    }
    response = requests.post(f"{API_BASE}/v1/query", json=payload, headers=auth_headers, timeout=30)
    assert response.status_code == 200, f"Query failed: {response.text}"
    
    result = response.json()
    assert len(result["results"]) >= 1, "Should find Alice's documents"
    print(f"âœ“ Alice's documents: {len(result['results'])} chunk(s)")
    
    # Bob's documents only
    payload["filters"] = {"uploaded_by": "bob@company.com"}
    response = requests.post(f"{API_BASE}/v1/query", json=payload, headers=auth_headers, timeout=30)
    assert response.status_code == 200
    
    result = response.json()
    assert len(result["results"]) >= 1, "Should find Bob's documents"
    print(f"âœ“ Bob's documents: {len(result['results'])} chunk(s)")


@pytest.mark.metadata_filter
@pytest.mark.e2e
def test_04b_metadata_filter_by_department(auth_headers):
    """Step 4b: Metadata filtering - department filtering"""
    print("\n=== Step 4b: Metadata filtering - Department ===")
    
    # Engineering department
    payload = {
        "query": "technical systems",
        "filters": {"department": "engineering"},
        "top_k": 10
    }
    response = requests.post(f"{API_BASE}/v1/query", json=payload, headers=auth_headers, timeout=30)
    assert response.status_code == 200
    
    result = response.json()
    assert len(result["results"]) >= 1, "Should find engineering documents"
    print(f"âœ“ Engineering: {len(result['results'])} chunk(s)")
    
    # Finance department
    payload["filters"] = {"department": "finance"}
    response = requests.post(f"{API_BASE}/v1/query", json=payload, headers=auth_headers, timeout=30)
    assert response.status_code == 200
    
    result = response.json()
    assert len(result["results"]) >= 1, "Should find finance documents"
    print(f"âœ“ Finance: {len(result['results'])} chunk(s)")


@pytest.mark.metadata_filter
@pytest.mark.e2e
def test_04c_metadata_filter_by_tags(auth_headers):
    """Step 4c: Metadata filtering - tag matching with $in operator"""
    print("\n=== Step 4c: Metadata filtering - Tags ($in) ===")
    
    # Python-related documents
    payload = {
        "query": "programming",
        "filters": {"tags": {"$in": ["python"]}},
        "top_k": 10
    }
    response = requests.post(f"{API_BASE}/v1/query", json=payload, headers=auth_headers, timeout=30)
    assert response.status_code == 200
    
    result = response.json()
    assert len(result["results"]) >= 1, "Should find Python documents"
    print(f"âœ“ Python tag: {len(result['results'])} chunk(s)")
    
    # Database-related documents
    payload["filters"] = {"tags": {"$in": ["database", "postgresql"]}}
    response = requests.post(f"{API_BASE}/v1/query", json=payload, headers=auth_headers, timeout=30)
    assert response.status_code == 200
    
    result = response.json()
    assert len(result["results"]) >= 1, "Should find database documents"
    print(f"âœ“ Database tags: {len(result['results'])} chunk(s)")


@pytest.mark.metadata_filter
@pytest.mark.e2e
def test_04d_metadata_filter_by_priority(auth_headers):
    """Step 4d: Metadata filtering - priority with $ne operator"""
    print("\n=== Step 4d: Metadata filtering - Priority ($ne) ===")
    
    # High priority only
    payload = {
        "query": "important",
        "filters": {"priority": "high"},
        "top_k": 10
    }
    response = requests.post(f"{API_BASE}/v1/query", json=payload, headers=auth_headers, timeout=30)
    assert response.status_code == 200
    
    result = response.json()
    assert len(result["results"]) >= 1, "Should find high priority documents"
    print(f"âœ“ High priority: {len(result['results'])} chunk(s)")
    
    # Non-high priority (medium or low)
    payload["filters"] = {"priority": {"$ne": "high"}}
    response = requests.post(f"{API_BASE}/v1/query", json=payload, headers=auth_headers, timeout=30)
    assert response.status_code == 200
    
    result = response.json()
    assert len(result["results"]) >= 1, "Should find non-high priority documents"
    print(f"âœ“ Non-high priority: {len(result['results'])} chunk(s)")


@pytest.mark.metadata_filter
@pytest.mark.e2e
def test_04e_metadata_filter_complex_and(auth_headers):
    """Step 4e: Metadata filtering - complex $and logic"""
    print("\n=== Step 4e: Metadata filtering - Complex AND ===")
    
    # Engineering department + Python tag + high priority
    payload = {
        "query": "technical guide",
        "filters": {
            "$and": [
                {"department": "engineering"},
                {"tags": {"$in": ["python"]}},
                {"priority": "high"}
            ]
        },
        "top_k": 10
    }
    response = requests.post(f"{API_BASE}/v1/query", json=payload, headers=auth_headers, timeout=30)
    assert response.status_code == 200
    
    result = response.json()
    assert len(result["results"]) >= 1, "Should find engineering + Python + high priority"
    print(f"âœ“ Engineering + Python + High priority: {len(result['results'])} chunk(s)")


@pytest.mark.metadata_filter
@pytest.mark.e2e
def test_04f_metadata_filter_complex_or(auth_headers):
    """Step 4f: Metadata filtering - complex $or logic"""
    print("\n=== Step 4f: Metadata filtering - Complex OR ===")
    
    # Finance OR Legal departments
    payload = {
        "query": "compliance business",
        "filters": {
            "$or": [
                {"department": "finance"},
                {"department": "legal"}
            ]
        },
        "top_k": 10
    }
    response = requests.post(f"{API_BASE}/v1/query", json=payload, headers=auth_headers, timeout=30)
    assert response.status_code == 200
    
    result = response.json()
    assert len(result["results"]) >= 1, "Should find finance or legal documents"
    print(f"âœ“ Finance OR Legal: {len(result['results'])} chunk(s)")


@pytest.mark.metadata_filter
@pytest.mark.e2e
def test_04g_metadata_filter_complex_not(auth_headers):
    """Step 4g: Metadata filtering - complex $not logic"""
    print("\n=== Step 4g: Metadata filtering - Complex NOT ===")
    
    # Exclude marketing department
    payload = {
        "query": "business documentation",
        "filters": {
            "$not": {"department": "marketing"}
        },
        "top_k": 10
    }
    response = requests.post(f"{API_BASE}/v1/query", json=payload, headers=auth_headers, timeout=30)
    assert response.status_code == 200
    
    result = response.json()
    assert len(result["results"]) >= 1, "Should find non-marketing documents"
    print(f"âœ“ NOT marketing: {len(result['results'])} chunk(s)")


@pytest.mark.metadata_filter
@pytest.mark.e2e
def test_04h_metadata_filter_nested(auth_headers):
    """Step 4h: Metadata filtering - deeply nested logic (agent use case)"""
    print("\n=== Step 4h: Metadata filtering - Deeply nested (agent scenario) ===")
    
    # Complex: (Alice OR Bob) AND high priority AND (python OR database tags)
    payload = {
        "query": "technical information",
        "filters": {
            "$and": [
                {
                    "$or": [
                        {"uploaded_by": "alice@company.com"},
                        {"uploaded_by": "bob@company.com"}
                    ]
                },
                {"priority": "high"},
                {
                    "$or": [
                        {"tags": {"$in": ["python"]}},
                        {"tags": {"$in": ["database"]}}
                    ]
                }
            ]
        },
        "top_k": 10
    }
    response = requests.post(f"{API_BASE}/v1/query", json=payload, headers=auth_headers, timeout=30)
    assert response.status_code == 200
    
    result = response.json()
    # May return 0 or more results depending on exact metadata match
    print(f"âœ“ Complex nested filter: {len(result['results'])} chunk(s)")


@pytest.mark.metadata_filter
@pytest.mark.e2e
def test_04i_metadata_filter_no_filter(auth_headers):
    """Step 4i: Metadata filtering - no filter returns all documents"""
    print("\n=== Step 4i: Metadata filtering - No filter (baseline) ===")
    
    payload = {
        "query": "information systems",
        "top_k": 20
    }
    response = requests.post(f"{API_BASE}/v1/query", json=payload, headers=auth_headers, timeout=30)
    assert response.status_code == 200
    
    result = response.json()
    assert len(result["results"]) >= 3, "Should find multiple documents across all users"
    print(f"âœ“ No filter (all documents): {len(result['results'])} chunk(s)")


@pytest.mark.semantic_search
@pytest.mark.e2e
def test_05a_semantic_search_products(auth_headers):
    """Step 5a: Semantic search - product queries should retrieve electronics catalog"""
    print("\n=== Step 5a: Semantic search - Product specifications ===")
    
    query = "Which smartphone model has the highest camera megapixels and what is its price?"
    response = requests.post(
        f"{API_BASE}/v1/query",
        json={"query": query, "top_k": 5, "min_similarity": 0.5},
        headers=auth_headers,
        timeout=30
    )
    
    assert response.status_code == 200, f"Query failed: {response.text}"
    result = response.json()
    
    assert result["total"] > 0, "No results returned for product query"
    assert len(result["results"]) > 0, "Empty results list"
    
    # Semantic validation: results should come from electronics_catalog.json
    filenames = [chunk["filename"] for chunk in result["results"]]
    assert any("electronics_catalog" in fn for fn in filenames), \
        f"Expected electronics_catalog in results, got: {filenames}"
    
    # Content validation: should mention camera specs and prices
    top_chunk = result["results"][0]["chunk_text"]
    assert any(keyword in top_chunk.lower() for keyword in ["camera", "megapixel", "mp", "photo"]), \
        "Expected camera-related content in top result"
    
    print(f"âœ“ Query: '{query}'")
    print(f"  Found {result['total']} chunks (top from: {result['results'][0]['filename']})")
    print(f"  Similarity: {result['results'][0]['similarity']:.3f}")
    print(f"  Semantic validation: PASSED (electronics catalog retrieved)")


@pytest.mark.semantic_search
@pytest.mark.e2e
def test_05b_semantic_search_art(auth_headers):
    """Step 5b: Semantic search - art queries should retrieve exhibition info"""
    print("\n=== Step 5b: Semantic search - Art exhibitions ===")
    
    query = "How much do family tickets cost for the art exhibition?"
    response = requests.post(
        f"{API_BASE}/v1/query",
        json={"query": query, "top_k": 5, "min_similarity": 0.5},
        headers=auth_headers,
        timeout=30
    )
    
    assert response.status_code == 200, f"Query failed: {response.text}"
    result = response.json()
    
    assert result["total"] > 0, "No results returned for art query"
    
    # Semantic validation: results should come from art_exhibition.html
    filenames = [chunk["filename"] for chunk in result["results"]]
    assert any("art_exhibition" in fn for fn in filenames), \
        f"Expected art_exhibition in results, got: {filenames}"
    
    # Content validation: should mention tickets/prices/family
    top_chunk = result["results"][0]["chunk_text"]
    assert any(keyword in top_chunk.lower() for keyword in ["ticket", "price", "family", "Â£", "exhibition"]), \
        "Expected art/ticket-related content in top result"
    
    print(f"âœ“ Query: '{query}'")
    print(f"  Found {result['total']} chunks (top from: {result['results'][0]['filename']})")
    print(f"  Similarity: {result['results'][0]['similarity']:.3f}")
    print(f"  Semantic validation: PASSED (art exhibition retrieved)")


@pytest.mark.semantic_search
@pytest.mark.e2e
def test_05c_semantic_search_business(auth_headers):
    """Step 5c: Semantic search - business queries should retrieve metrics"""
    print("\n=== Step 5c: Semantic search - Business metrics ===")
    
    query = "What is the customer satisfaction rating and which region has the highest sales growth?"
    response = requests.post(
        f"{API_BASE}/v1/query",
        json={"query": query, "top_k": 5, "min_similarity": 0.5},
        headers=auth_headers,
        timeout=30
    )
    
    assert response.status_code == 200, f"Query failed: {response.text}"
    result = response.json()
    
    assert result["total"] > 0, "No results returned for business query"
    
    # Semantic validation: results should come from business_metrics.yaml
    filenames = [chunk["filename"] for chunk in result["results"]]
    assert any("business_metrics" in fn for fn in filenames), \
        f"Expected business_metrics in results, got: {filenames}"
    
    # Content validation: should mention CAC/LTV metrics
    top_chunk = result["results"][0]["chunk_text"]
    assert any(keyword in top_chunk.lower() for keyword in ["cac", "ltv", "customer", "acquisition", "value"]), \
        "Expected business metrics in top result"
    
    print(f"âœ“ Query: '{query}'")
    print(f"  Found {result['total']} chunks (top from: {result['results'][0]['filename']})")
    print(f"  Similarity: {result['results'][0]['similarity']:.3f}")
    print(f"  Semantic validation: PASSED (business metrics retrieved)")


@pytest.mark.semantic_search
@pytest.mark.e2e
def test_05d_semantic_search_compliance(auth_headers):
    """Step 5d: Semantic search - legal queries should retrieve GDPR report"""
    print("\n=== Step 5d: Semantic search - Legal compliance ===")
    
    query = "What are the critical GDPR compliance findings in our assessment?"
    response = requests.post(
        f"{API_BASE}/v1/query",
        json={"query": query, "top_k": 5, "min_similarity": 0.5},
        headers=auth_headers,
        timeout=30
    )
    
    assert response.status_code == 200, f"Query failed: {response.text}"
    result = response.json()
    
    assert result["total"] > 0, "No results returned for compliance query"
    
    # Semantic validation: results should come from gdpr_compliance.xml
    filenames = [chunk["filename"] for chunk in result["results"]]
    assert any("gdpr_compliance" in fn for fn in filenames), \
        f"Expected gdpr_compliance in results, got: {filenames}"
    
    # Content validation: should mention GDPR/compliance/critical
    top_chunk = result["results"][0]["chunk_text"]
    assert any(keyword in top_chunk.lower() for keyword in ["gdpr", "compliance", "critical", "finding", "assessment"]), \
        "Expected GDPR compliance content in top result"
    
    print(f"âœ“ Query: '{query}'")
    print(f"  Found {result['total']} chunks (top from: {result['results'][0]['filename']})")
    print(f"  Similarity: {result['results'][0]['similarity']:.3f}")
    print(f"  Semantic validation: PASSED (GDPR report retrieved)")


@pytest.mark.semantic_search
@pytest.mark.e2e
def test_05e_semantic_search_financials(auth_headers):
    """Step 5e: Semantic search - financial queries should retrieve quarterly report"""
    print("\n=== Step 5e: Semantic search - Financial data ===")
    
    query = "Show quarterly revenue growth trends over 2025"
    response = requests.post(
        f"{API_BASE}/v1/query",
        json={"query": query, "top_k": 5, "min_similarity": 0.5},
        headers=auth_headers,
        timeout=30
    )
    
    assert response.status_code == 200, f"Query failed: {response.text}"
    result = response.json()
    
    assert result["total"] > 0, "No results returned for financial query"
    
    # Semantic validation: results should come from financial_quarterly_report.csv
    filenames = [chunk["filename"] for chunk in result["results"]]
    assert any("financial" in fn or "quarterly" in fn for fn in filenames), \
        f"Expected financial report in results, got: {filenames}"
    
    # Content validation: should mention revenue/quarterly/2025
    top_chunk = result["results"][0]["chunk_text"]
    assert any(keyword in top_chunk.lower() for keyword in ["revenue", "quarter", "2025", "growth"]), \
        "Expected financial data in top result"
    
    print(f"âœ“ Query: '{query}'")
    print(f"  Found {result['total']} chunks (top from: {result['results'][0]['filename']})")
    print(f"  Similarity: {result['results'][0]['similarity']:.3f}")
    print(f"  Semantic validation: PASSED (financial report retrieved)")


@pytest.mark.semantic_search
@pytest.mark.e2e
def test_05f_semantic_search_operations(auth_headers):
    """Step 5f: Semantic search - ops queries should retrieve system logs"""
    print("\n=== Step 5f: Semantic search - System operations ===")
    
    # More specific query that better matches log file content
    query = "duplicate document hash collision error upload rejected"
    response = requests.post(
        f"{API_BASE}/v1/query",
        json={"query": query, "top_k": 5, "min_similarity": 0.3},  # Lower threshold for log files
        headers=auth_headers,
        timeout=30
    )
    
    assert response.status_code == 200, f"Query failed: {response.text}"
    result = response.json()
    
    assert result["total"] > 0, "No results returned for operations query"
    
    # Semantic validation: results should come from rag_system_operations.log
    filenames = [chunk["filename"] for chunk in result["results"]]
    assert any("operations" in fn or "rag_system" in fn for fn in filenames), \
        f"Expected operations log in results, got: {filenames}"
    
    # Content validation: should mention duplicate/rejection/error
    top_chunk = result["results"][0]["chunk_text"]
    assert any(keyword in top_chunk.lower() for keyword in ["duplicate", "reject", "error", "hash", "collision"]), \
        "Expected operations/error content in top result"
    
    print(f"âœ“ Query: '{query}'")
    print(f"  Found {result['total']} chunks (top from: {result['results'][0]['filename']})")
    print(f"  Similarity: {result['results'][0]['similarity']:.3f}")
    print(f"  Semantic validation: PASSED (operations log retrieved)")


@pytest.mark.semantic_search
@pytest.mark.e2e
def test_05g_semantic_isolation_negative(auth_headers):
    """Step 5g: Negative test - topic isolation (camera query should NOT return art/business docs)"""
    print("\n=== Step 5g: Semantic isolation test ===")
    
    query = "smartphone camera specifications"
    response = requests.post(
        f"{API_BASE}/v1/query",
        json={"query": query, "top_k": 10, "min_similarity": 0.5},
        headers=auth_headers,
        timeout=30
    )
    
    assert response.status_code == 200, f"Query failed: {response.text}"
    result = response.json()
    
    assert result["total"] > 0, "No results returned"
    
    # Get all results to check topic isolation
    all_filenames = [chunk["filename"] for chunk in result["results"]]
    
    # PRIMARY: Should retrieve electronics catalog in results (positive validation)
    assert any("electronics" in fn for fn in all_filenames), \
        f"Expected electronics_catalog in results for camera query, got: {all_filenames[:5]}"
    
    # SECONDARY: Among test documents, should NOT retrieve art/business/compliance docs
    # (Note: May have other user documents in DB, that's OK)
    test_docs_in_results = [fn for fn in all_filenames if any(
        topic in fn for topic in ["electronics_catalog", "art_exhibition", "business_metrics", 
                                   "gdpr_compliance", "financial_quarterly", "rag_system_operations",
                                   "rag_architecture", "google_agent", "vector_databases"]
    )]
    
    unwanted_test_topics = ["art_exhibition", "business_metrics", "gdpr_compliance"]
    unwanted_in_results = [fn for fn in test_docs_in_results if any(topic in fn for topic in unwanted_test_topics)]
    
    if unwanted_in_results:
        print(f"  WARNING: Unwanted test documents appeared: {unwanted_in_results}")
        print(f"  (This suggests embeddings aren't isolating topics well)")
    
    print(f"âœ“ Query: '{query}'")
    print(f"  Test documents in results: {test_docs_in_results[:3]}")
    print(f"  Semantic validation: PASSED (electronics_catalog retrieved)")


@pytest.mark.reranking
@pytest.mark.e2e
def test_05h_query_with_reranking(auth_headers):
    """Step 5h: Reranking fixes keyword confusion - compliance scores vs business performance metrics"""
    print("\n=== Step 5h: Reranking resolves keyword ambiguity ===")
    
    # Tricky query: "score", "performance", "improvement" appear across multiple contexts
    # - business_metrics.yaml: performance scores, growth targets, KPI improvements (WRONG - business)
    # - gdpr_compliance.xml: compliance scores, principle assessments, improvement recommendations (CORRECT - legal)
    # - financial_quarterly_report.csv: financial performance (WRONG - money)
    # 
    # Bi-encoder may prefer business metrics (more "performance" keyword density)
    # Cross-encoder should understand question is about "assessment" + "improvement areas" = compliance report
    query = "What are the current performance scores and which areas need improvement?"
    
    # Query WITHOUT reranking (baseline)
    response_no_rerank = requests.post(
        f"{API_BASE}/v1/query",
        json={
            "query": query,
            "top_k": 5,
            "rerank": False,
            "metadata_filter": {"category": "e2e-fixture"}
        },
        headers=auth_headers,
        timeout=30
    )
    assert response_no_rerank.status_code == 200
    results_no_rerank = response_no_rerank.json()["results"]
    
    # Query WITH reranking (Gemini LLM batch mode: single API call ~10-15s)
    response_rerank = requests.post(
        f"{API_BASE}/v1/query",
        json={
            "query": query,
            "top_k": 5,
            "rerank": True,
            "rerank_candidates": 10,
            "metadata_filter": {"category": "e2e-fixture"}
        },
        headers=auth_headers,
        timeout=30  # Batch mode: much faster
    )
    assert response_rerank.status_code == 200
    results_rerank = response_rerank.json()["results"]
    
    assert len(results_no_rerank) > 0, "No baseline results"
    assert len(results_rerank) > 0, "No reranked results"
    
    # Target: gdpr_compliance.xml (has data retention policies, personal information storage)
    target_filename = "gdpr_compliance.xml"
    
    baseline_pos = next((i for i, r in enumerate(results_no_rerank) if target_filename in r["filename"]), None)
    reranked_pos = next((i for i, r in enumerate(results_rerank) if target_filename in r["filename"]), None)
    
    # Results WITHOUT reranking should NOT have rerank_score
    for result in results_no_rerank:
        assert "rerank_score" not in result, "Baseline results should not have rerank_score"
    
    # Results WITH reranking SHOULD have rerank_score
    for result in results_rerank:
        assert "rerank_score" in result, "Reranked results must have rerank_score"
        assert isinstance(result["rerank_score"], (int, float))
    
    # Scores should be sorted descending
    scores = [r["rerank_score"] for r in results_rerank]
    assert scores == sorted(scores, reverse=True), "Rerank scores not in descending order"
    
    print(f"âœ“ Query: '{query}'")
    print(f"  Target document: {target_filename} (compliance assessment with scores/improvement areas)")
    
    # Show baseline rankings
    if len(results_no_rerank) > 0:
        print(f"  Baseline top 3:")
        for i, r in enumerate(results_no_rerank[:3]):
            marker = " â† TARGET" if target_filename in r["filename"] else ""
            print(f"    #{i+1}. {r['filename']} (similarity: {r['similarity']:.3f}){marker}")
    
    # Show reranked rankings
    if len(results_rerank) > 0:
        print(f"  Reranked top 3:")
        for i, r in enumerate(results_rerank[:3]):
            marker = " â† TARGET" if target_filename in r["filename"] else ""
            print(f"    #{i+1}. {r['filename']} (rerank_score: {r['rerank_score']:.3f}){marker}")
    
    if baseline_pos is not None and reranked_pos is not None:
        # Validate reranking improved or maintained position
        assert reranked_pos <= baseline_pos, f"Reranking made position worse: {baseline_pos + 1} â†’ {reranked_pos + 1}"
        if reranked_pos < baseline_pos:
            print(f"  âœ… Position IMPROVED: #{baseline_pos + 1} â†’ #{reranked_pos + 1} (+{baseline_pos - reranked_pos} positions)")
            print(f"  ðŸŽ¯ Cross-encoder understood legal compliance context (not business metrics)")
        else:
            print(f"  âœ… Position maintained at #{reranked_pos + 1}")
    elif reranked_pos is not None:
        print(f"  âœ… Target found at #{reranked_pos + 1} (not in baseline top 5)")
        print(f"  ðŸŽ¯ Reranking discovered GDPR doc that bi-encoder missed!")
    else:
        print(f"  âš ï¸ Target not in top 5 (baseline: {'#' + str(baseline_pos + 1) if baseline_pos is not None else 'none'})") 
    
    print(f"  Reranking validation: PASSED")


@pytest.mark.reranking
@pytest.mark.e2e
def test_05i_reranking_improves_relevance(auth_headers):
    """Step 5i: Deterministic keyword trap - reranking chooses technical doc over fairy tale"""
    print("\n=== Step 5i: Keyword trap test (deterministic) ===")
    
    # SETUP: Two uploaded documents:
    # - red_riding_hood_story.txt: fairy tale that mentions "hybrid search", "RAG", "algorithms" (keyword trap!)
    # - hybrid_search_technical.txt: actual technical explanation of hybrid search algorithms
    
    # Query asking for technical explanation
    query = "How do hybrid search algorithms work in RAG systems? Explain the technical implementation."
    
    # Query WITHOUT reranking (vector search may prefer keyword-dense story)
    response_baseline = requests.post(
        f"{API_BASE}/v1/query",
        json={
            "query": query,
            "top_k": 10,
            "rerank": False,
            "metadata_filter": {"category": "e2e-fixture"}
        },
        headers=auth_headers,
        timeout=30
    )
    assert response_baseline.status_code == 200
    baseline_results = response_baseline.json()["results"]
    
    # Query WITH reranking (LLM should understand semantic relevance)
    response_rerank = requests.post(
        f"{API_BASE}/v1/query",
        json={
            "query": query,
            "top_k": 10,
            "rerank": True,
            "rerank_candidates": 20,
            "metadata_filter": {"category": "e2e-fixture"}
        },
        headers=auth_headers,
        timeout=30
    )
    assert response_rerank.status_code == 200
    reranked_results = response_rerank.json()["results"]
    
    assert len(baseline_results) > 0, "No baseline results"
    assert len(reranked_results) > 0, "No reranked results"
    
    # Find document positions
    technical_doc = "hybrid_search_technical.txt"
    story_doc = "red_riding_hood_story.txt"
    
    baseline_technical_pos = next((i for i, r in enumerate(baseline_results) if technical_doc in r["filename"]), None)
    baseline_story_pos = next((i for i, r in enumerate(baseline_results) if story_doc in r["filename"]), None)
    
    reranked_technical_pos = next((i for i, r in enumerate(reranked_results) if technical_doc in r["filename"]), None)
    reranked_story_pos = next((i for i, r in enumerate(reranked_results) if story_doc in r["filename"]), None)
    
    # Validate rerank_score presence
    for result in reranked_results:
        assert "rerank_score" in result, "Missing rerank_score"
        assert isinstance(result["rerank_score"], (int, float))
    
    # Scores sorted descending
    scores = [r["rerank_score"] for r in reranked_results]
    assert scores == sorted(scores, reverse=True), "Results not sorted by rerank_score"
    
    print(f"âœ“ Query: '{query}'")
    print(f"  Documents: {technical_doc} (technical) vs {story_doc} (keyword trap)")
    
    # Show baseline results
    print(f"\n  Baseline (vector search) top 5:")
    for i, r in enumerate(baseline_results[:5]):
        marker = ""
        if technical_doc in r["filename"]:
            marker = " â† TECHNICAL DOC"
        elif story_doc in r["filename"]:
            marker = " â† STORY (keyword trap)"
        print(f"    #{i+1}. {r['filename']} (similarity: {r.get('similarity', 0):.3f}){marker}")
    
    # Show reranked results  
    print(f"\n  Reranked (LLM) top 5:")
    for i, r in enumerate(reranked_results[:5]):
        marker = ""
        if technical_doc in r["filename"]:
            marker = " â† TECHNICAL DOC"
        elif story_doc in r["filename"]:
            marker = " â† STORY (keyword trap)"
        print(f"    #{i+1}. {r['filename']} (rerank_score: {r['rerank_score']:.3f}){marker}")
    
    # DETERMINISTIC ASSERTION:
    # After reranking, technical doc MUST have higher score than story (if both found)
    if reranked_technical_pos is not None and reranked_story_pos is not None:
        assert reranked_technical_pos < reranked_story_pos, \
            f"Reranking failed: technical doc at #{reranked_technical_pos+1}, story at #{reranked_story_pos+1} (technical should be higher!)"
        print(f"\n  âœ… PASS: Technical doc (#{reranked_technical_pos+1}) ranked higher than story (#{reranked_story_pos+1})")
    elif reranked_technical_pos is not None:
        print(f"\n  âœ… PASS: Technical doc found at #{reranked_technical_pos+1}, story not in results")
    else:
        raise AssertionError("Technical doc not found in reranked results!")
    
    print(f"  Keyword trap test: PASSED")


@pytest.mark.reranking
@pytest.mark.e2e
def test_05j_reranking_performance(auth_headers):
    """Step 5j: Verify reranking completes in reasonable time"""
    print("\n=== Step 5j: Reranking performance test ===")
    
    import time
    start = time.time()
    
    # Gemini LLM with parallel batching: 20 docs = 2 batches Ã— 10 docs (~10-15s)
    response = requests.post(
        f"{API_BASE}/v1/query",
        json={
            "query": "database vector search similarity",
            "top_k": 5,
            "rerank": True,
            "rerank_candidates": 20
        },
        headers=auth_headers,
        timeout=30  # Parallel batching: much faster
    )
    
    elapsed = time.time() - start
    
    assert response.status_code == 200
    assert len(response.json()["results"]) > 0
    
    # Parallel batching should be fast (< 20s for 20 docs with 2 parallel calls)
    assert elapsed < 20.0, f"Reranking took {elapsed:.2f}s (expected < 20s)"
    
    print(f"âœ“ Reranking completed in {elapsed:.2f}s")
    print(f"  Performance validation: PASSED")


@pytest.mark.download
@pytest.mark.e2e
def test_06_download_document():
    """Step 6: Download original document"""
    print("\n=== Step 6: Download original document ===")
    
    # Get first document
    response = requests.get(f"{API_BASE}/v1/documents", timeout=10)
    docs = response.json()["documents"]
    
    if not docs:
        pytest.skip("No documents to download")
    
    doc_id = docs[0]["doc_id"]
    filename = docs[0]["filename"]
    
    response = requests.get(f"{API_BASE}/v1/documents/{doc_id}/download", timeout=30)
    assert response.status_code == 200, f"Download failed: {response.text}"
    
    content = response.content
    assert len(content) > 0, "Downloaded file is empty"
    
    print(f"âœ“ Downloaded: {filename}")
    print(f"  Size: {len(content):,} bytes")
    print(f"  Content-Type: {response.headers.get('content-type')}")


@pytest.mark.storage
@pytest.mark.e2e
def test_07_verify_gcs_storage(gcs_client):
    """Step 7: Verify GCS contains all uploaded files"""
    print("\n=== Step 7: Verify GCS storage ===")
    
    bucket = gcs_client.bucket(GCS_BUCKET)
    blobs = list(bucket.list_blobs())
    
    assert len(blobs) > 0, "GCS bucket is empty"
    
    # Group by UUID
    uuids = set()
    file_types = {}
    
    for blob in blobs:
        uuid = blob.name.split('/')[0]
        uuids.add(uuid)
        file_type = blob.name.split('/')[-1]
        file_types[file_type] = file_types.get(file_type, 0) + 1
    
    print(f"âœ“ GCS contains {len(blobs)} objects for {len(uuids)} document(s)")
    print(f"  File types: {dict(file_types)}")


@pytest.mark.cleanup
@pytest.mark.e2e
def test_99_cleanup_after(test_documents, auth_headers):
    """Step 99: Post-cleanup - Remove test documents after suite"""
    print("\n=== Step 99: Post-cleanup (removes test documents) ===")
    print("ðŸ§¹ Cleaning up test documents after tests...")
    print("   (user documents will be preserved)")
    
    deleted = 0
    for hash_key in ["txt_hash", "pdf_hash", "md_hash", "json_hash", "html_hash", "yaml_hash", "xml_hash", "csv_hash", "log_hash", "security_hash", "technical_hash", "story_hash"]:
        file_hash = test_documents[hash_key]
        response = requests.delete(f"{API_BASE}/v1/documents/by-hash/{file_hash}", headers=auth_headers, timeout=30)
        if response.status_code == 200:
            deleted += 1
    
    print(f"âœ“ Deleted {deleted} test document(s)")
    print("âœ“ User documents preserved")
    print("\nðŸŽ‰ E2E test completed successfully!")


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s"])
