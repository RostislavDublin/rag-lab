# Development Guide

This guide covers local development setup, configuration, and workflows for RAG Lab.

## Configuration Files

The project uses **three different configuration files** for different environments:

### 1. `.env.local` (Local Development)

**Purpose:** Development on localhost with uvicorn  
**Database:** Cloud SQL Proxy or local PostgreSQL  
**Auth:** OAuth tokens for testing, optional JWT validation  
**Template:** [.env.local.example](../.env.local.example)

**Use when:**
- Running `uvicorn src.main:app --reload` directly
- Developing features locally
- Running tests with real GCP services

```bash
# Copy template
cp .env.local.example .env.local
# Edit with your values
```

### 2. `.env` (Cloud Run Deployment)

**Purpose:** Production deployment to Cloud Run  
**Database:** Cloud SQL via Unix socket  
**Auth:** Service account from Cloud Run metadata  
**Template:** [.env.example](../.env.example)  
**Generated by:** `deployment/setup_infrastructure.py`

**Use when:**
- Deploying to Cloud Run with `gcloud run deploy`
- Running in GCP production environment
- Service account has all necessary permissions

**Note:** This file is auto-generated during infrastructure setup. Do not create manually.

### 3. `.env.deploy` (Deployment Configuration)

**Purpose:** Infrastructure setup and deployment process parameters (NOT runtime config)  
**Contains:** GCP project/region, resource names (buckets, instances), Cloud Run sizing (memory/CPU)  
**Does NOT contain:** Application config (embedding models, reranking, auth, logging)  
**Template:** [deployment/.env.deploy.example](../deployment/.env.deploy.example)

**Use when:**
- Running `deployment/setup_infrastructure.sh` or `setup_infrastructure.py`
- Configuring Cloud Run resource limits (memory, CPU, instances)
- Setting up GCP resources for first time

**Example parameters:**
- `GCP_PROJECT_ID`, `GCP_REGION`
- `GCS_BUCKET`, `CLOUD_SQL_INSTANCE`, `SERVICE_ACCOUNT_NAME`
- `CLOUD_RUN_MEMORY`, `CLOUD_RUN_CPU`, `CLOUD_RUN_MAX_INSTANCES`

```bash
# Copy template
cp deployment/.env.deploy.example deployment/.env.deploy
# Edit with your GCP project settings
```

---

**Quick Reference:**

| File | Purpose | What It Contains | When to Use |
|------|---------|------------------|-------------|
| `.env.local` | Local development | App config + localhost DB connection | `uvicorn` locally |
| `.env` | Production runtime | App config + Cloud SQL socket | Deployed service in Cloud Run |
| `.env.deploy` | Deployment process | Infrastructure settings (no app config) | `setup_infrastructure.sh` |

**Key Difference:**
- `.env.local` and `.env` = **application configuration** (embedding models, reranking, JWT auth, logging)
- `.env.deploy` = **infrastructure configuration** (project ID, resource names, Cloud Run sizing)

---

## Local Development Setup

### Option 1: uvicorn with Hot Reload (Recommended)

**Prerequisites:**

1. **Create `.env.local` file** (required!):
   ```bash
   # Create .env.local with required variables:
   cat > .env.local << 'EOF'
# Database (Cloud SQL via Proxy or local PostgreSQL)
DATABASE_URL=postgresql://user:password@localhost:5432/rag_db

# GCP Configuration
GCP_PROJECT_ID=your-project-id
GCP_REGION=us-central1
GCS_BUCKET=your-project-id-rag-documents

# Vertex AI
VERTEX_AI_LOCATION=us-central1

# FastAPI
PORT=8080
EOF
   ```

2. **Setup virtual environment** (one-time):
   ```bash
   python3 -m venv .venv
   source .venv/bin/activate
   pip install -r requirements.txt
   ```

**Start Server:**

```bash
# Activate virtual environment
source .venv/bin/activate

# Start uvicorn with hot reload
uvicorn src.main:app --host 0.0.0.0 --port 8080 --reload

# Server runs at http://localhost:8080
# Swagger UI at http://localhost:8080/docs
# Code changes auto-reload (no restart needed!)
```

**One-liner (from project root):**
```bash
source .venv/bin/activate && uvicorn src.main:app --host 0.0.0.0 --port 8080 --reload
```

**Cloud SQL Proxy Setup (for Cloud SQL connection):**

If using Cloud SQL (not local PostgreSQL), start Cloud SQL Proxy in background:

```bash
# Start Cloud SQL Proxy on localhost:5432 (TCP, not unix socket)
# Survives terminal close, logs to /tmp/cloud-sql-proxy.log
nohup cloud-sql-proxy PROJECT_ID:REGION:INSTANCE_NAME > /tmp/cloud-sql-proxy.log 2>&1 &

# Example:
nohup cloud-sql-proxy myai-475419:us-central1:rag-postgres > /tmp/cloud-sql-proxy.log 2>&1 &

# Check proxy is running:
ps aux | grep cloud-sql-proxy

# View logs:
tail -f /tmp/cloud-sql-proxy.log

# Stop proxy:
killall cloud-sql-proxy
```

**DATABASE_URL for Cloud SQL Proxy:**
```bash
# .env.local - LOCAL DEVELOPMENT (via Cloud SQL Proxy on localhost:5432)
DATABASE_URL=postgresql+asyncpg://user:password@localhost:5432/dbname

# .env - CLOUD RUN PRODUCTION (via unix socket)
DATABASE_URL=postgresql+asyncpg://user:password@/dbname?host=/cloudsql/PROJECT_ID:REGION:INSTANCE
```

**Note:** Local development uses `@localhost:5432` (TCP via proxy), Cloud Run uses `@/dbname?host=/cloudsql/...` (unix socket).

### Option 2: Deployment Script

Fast iteration with Cloud SQL Proxy and hot reload:

```bash
# 1. Setup infrastructure (one-time)
cd deployment
cp .env.deploy.example .env.deploy
# Edit .env.deploy with your GCP_PROJECT_ID and GCP_REGION

# Create GCP resources (Cloud SQL, GCS, Service Account)
python setup_infrastructure.py

# 2. Start local development server
python local_run.py

# Server runs at http://localhost:8080 with hot reload
# Connects to Cloud SQL via proxy
# Uses real Vertex AI embeddings
# Stores documents in GCS
```

**Benefits:**
- Hot reload: code changes apply instantly
- Real infrastructure: same as production
- Fast iteration: no Docker builds
- Cloud SQL Proxy: automatic connection

### Option 3: Docker Compose (Fully Local)

```bash
# Start PostgreSQL + API
docker-compose up -d

# View logs
docker-compose logs -f api

# Stop services
docker-compose down
```

## Environment Variables Reference

### Required Variables

| Variable | Description | Example |
|----------|-------------|---------|
| `GCP_PROJECT_ID` | Google Cloud project ID | `myproject-12345` |
| `GCS_BUCKET` | Cloud Storage bucket name | `myproject-rag-documents` |
| `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@localhost:5432/rag_db` |

### Optional Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `GCP_LOCATION` | `us-central1` | Vertex AI region |
| `PORT` | `8080` | Server port |
| `LOG_LEVEL` | `INFO` | Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL) |
| `GCS_CONNECTION_POOL_SIZE` | `10` | Max concurrent GCS operations |
| `GOOGLE_APPLICATION_CREDENTIALS` | - | Path to service account key (optional for local dev with gcloud auth) |

### Authentication Variables

| Variable | Description | Example |
|----------|-------------|---------|
| `ALLOWED_USERS` | Whitelist of allowed user emails (comma-separated) | `alice@company.com,bob@company.com` or `*` (all authenticated) |
| `TRUSTED_SERVICE_ACCOUNTS` | Service accounts allowed to use X-End-User-ID header | `sa@project.iam.gserviceaccount.com` |
| `JWKS_URI` | JWKS endpoint for JWT validation | `https://www.googleapis.com/oauth2/v3/certs` |

### Reranking Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `RERANKER_ENABLED` | **REQUIRED** | Enable reranking (true/false) |
| `RERANKER_TYPE` | **REQUIRED** | Reranker type: `gemini`, `local`, or `cohere` |
| `RERANKER_MODEL` | **REQUIRED** | Model name: `gemini-2.5-flash-lite` (recommended) |
| `LLM_EXTRACTION_MODEL` | **REQUIRED** | LLM for summary/keywords extraction: `gemini-2.5-flash-lite` |
| `GEMINI_RERANK_BATCH_SIZE` | `2` | Documents per batch for Gemini reranking |
| `GEMINI_RERANK_MAX_CONCURRENT` | `10` | Max parallel batches |

**Note:** No default values in code - all variables must be set explicitly in `.env.local`

See [docs/reranking.md](reranking.md) for detailed reranking configuration.

## Logging Configuration

The application uses Python's standard logging module with configurable levels and automatic log rotation:

**Log Configuration:**
- **Console logging**: Always enabled (colored output in development)
- **File logging**: Rotating log files with automatic retention
- **Log location**: `logs/rag-lab_YYYYMMDD_HHMMSS.log`
- **Rotation**: 10MB per file, keeps last 10 files
- **New session**: Each server restart creates timestamped log file

**Log Levels:**
- `DEBUG`: Detailed trace (chunk sizes, file processing, reranking batches)
- `INFO`: Important operations (document processing, chunking summary, reranking timing)
- `WARNING`: Recoverable issues (UTF-8 decode failures, GCS connection pool warnings)
- `ERROR`: Failures requiring attention

**Configuration:**
```bash
# In .env.local
LOG_LEVEL=INFO  # Production default (console + file)
LOG_LEVEL=DEBUG # For troubleshooting (verbose file logging)
```

**Third-party library logs are suppressed** (httpx, google, urllib3, asyncio) to reduce noise - only WARNING and above are shown.

**Example log files:**
```
logs/
├── rag-lab_20251216_143025.log  # Session 1 (10MB)
├── rag-lab_20251216_143025.log.1  # Rotated backup
├── rag-lab_20251216_165815.log  # Session 2 (current)
└── ...
```

**Example output (INFO level with reranking):**
```
2025-12-16 16:58:15 - src.main - INFO - Initializing Google Gen AI (project=myai-475419, location=us-central1)...
2025-12-16 16:58:15 - src.main - INFO - Google Gen AI client initialized successfully
2025-12-16 16:58:15 - src.database - INFO - Connected to PostgreSQL: localhost:5432/rag_db
2025-12-16 16:58:16 - src.reranking.gemini - INFO - Reranking 20 documents with Gemini (gemini-2.5-flash-lite) - BATCH MODE (batches of 2)
2025-12-16 16:58:16 - src.reranking.gemini - INFO - ⏱️ Batch 1/10 START (2 docs)
2025-12-16 16:58:16 - src.reranking.gemini - INFO - ⏱️ Batch 2/10 START (2 docs)
...
2025-12-16 16:58:23 - src.reranking.gemini - INFO - ✅ Batch 1/10 DONE in 7.12s
2025-12-16 16:58:23 - src.reranking.gemini - INFO - ✅ Batch 10/10 DONE in 7.24s
2025-12-16 16:58:23 - src.reranking.gemini - INFO - Reranking complete. Top score: 1.000, Bottom score: 0.000
```

## System Dependencies

### File Validation (magic bytes detection)

**macOS:**
```bash
brew install libmagic
```

**Ubuntu/Debian:**
```bash
sudo apt-get install libmagic1
```

**Why needed:** The `python-magic` library wraps the system `libmagic` library to perform industry-standard file type detection using magic bytes (first 2KB of file). This prevents file spoofing attacks and ensures correct processing pipelines.

## Development Workflow

### 1. Local Development

```bash
# Start services
docker-compose up -d

# Edit code (hot reload enabled)
vim src/main.py

# View logs
docker-compose logs -f api

# Test changes
curl http://localhost:8080/health
```

### 2. Database Access

```bash
# Connect to PostgreSQL
docker exec -it raglab-postgres psql -U raglab -d raglab

# Query documents (metadata only, no file content)
SELECT id, doc_uuid, filename, chunk_count, uploaded_at FROM original_documents;

# Query chunks (embeddings only, no text)
SELECT COUNT(*) FROM document_chunks;
SELECT original_doc_id, chunk_index FROM document_chunks LIMIT 5;

# Vector search test (returns chunk_index to fetch from GCS)
SELECT dc.chunk_index, od.doc_uuid, 
       1 - (dc.embedding <=> '[0,0,...]'::vector) as similarity
FROM document_chunks dc
JOIN original_documents od ON dc.original_doc_id = od.id
ORDER BY dc.embedding <=> '[0,0,...]'::vector
LIMIT 3;

# Check GCS paths
SELECT doc_uuid, 
       CONCAT('gs://raglab-documents/', doc_uuid, '/document.pdf') as pdf_path,
       CONCAT('gs://raglab-documents/', doc_uuid, '/chunks/000.json') as chunk_path
FROM original_documents;
```

### 3. Database Connection String Formats

```bash
# Cloud SQL with private IP
DATABASE_URL=postgresql://user:pass@10.1.2.3:5432/raglab

# Cloud SQL with Unix socket
DATABASE_URL=postgresql://user:pass@/cloudsql/project:region:instance/raglab

# Local development
DATABASE_URL=postgresql://raglab:password@localhost:5432/raglab
```

## Testing

See [testing.md](testing.md) for comprehensive testing guide.

## Troubleshooting

### Database Connection Failed

```bash
# Check PostgreSQL is running
docker-compose ps

# Check logs
docker-compose logs postgres

# Test connection
psql $DATABASE_URL
```

### Vertex AI Authentication Failed

```bash
# Check credentials
echo $GOOGLE_APPLICATION_CREDENTIALS
cat $GOOGLE_APPLICATION_CREDENTIALS

# Set project ID
export GCP_PROJECT_ID=your-project-id

# Test gcloud auth
gcloud auth list
```

### libmagic Installation Issues

**macOS - "failed to find libmagic":**
```bash
brew install libmagic
# If still fails, try:
brew reinstall libmagic
```

**Linux - "failed to find libmagic":**
```bash
sudo apt-get update
sudo apt-get install libmagic1 libmagic-dev
```

**Docker:** Add to Dockerfile:
```dockerfile
RUN apt-get update && apt-get install -y libmagic1
```
